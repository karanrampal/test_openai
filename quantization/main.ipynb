{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Python 3.9.18\n",
    "#accelerate==0.26.1\n",
    "#seaborn==0.13.1\n",
    "#torch==2.1.1\n",
    "#transformers==4.35.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quantization I: Asymmetric mode\n",
    "Quantize and De-quantize a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization with Random Scale and Zero Point\n",
    "- Implement Linear Quantization for when the \"scale\" and the \"zero point\" are known/randomly selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_q_with_scale_and_zero_point(tensor, scale, zero_point, dtype = torch.int8):\n",
    "\n",
    "    scaled_and_shifted_tensor = tensor / scale + zero_point\n",
    "\n",
    "    rounded_tensor = torch.round(scaled_and_shifted_tensor)\n",
    "\n",
    "    q_min = torch.iinfo(dtype).min\n",
    "    q_max = torch.iinfo(dtype).max\n",
    "\n",
    "    q_tensor = rounded_tensor.clamp(q_min,q_max).to(dtype)\n",
    "    \n",
    "    return q_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### a dummy tensor to test the implementation\n",
    "test_tensor=torch.tensor(\n",
    "    [[191.6, -13.5, 728.6],\n",
    "     [92.14, 295.5,  -184],\n",
    "     [0,     684.6, 245.5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### these are random values for \"scale\" and \"zero_point\" to test the implementation\n",
    "scale = 3.5\n",
    "zero_point = -70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, scale, zero_point)\n",
    "quantized_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dequantization with Random Scale and Zero Point\n",
    "- Now, Dequantize the tensor to see how precise the quantization is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantized_tensor = scale * (quantized_tensor.float() - zero_point)\n",
    "dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### without casting to float\n",
    "scale * (quantized_tensor - zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_dequantization(quantized_tensor, scale, zero_point):\n",
    "    return scale * (quantized_tensor.float() - zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantized_tensor = linear_dequantization(quantized_tensor, scale, zero_point)\n",
    "dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=None):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of tensors using seaborn\n",
    "    \"\"\"\n",
    "    sns.heatmap(tensor.cpu().numpy(), ax=ax, vmin=vmin, vmax=vmax, cmap=cmap, annot=True, fmt=\".2f\", cbar=False)\n",
    "    ax.set_title(title)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "\n",
    "\n",
    "def plot_quantization_errors(original_tensor, quantized_tensor, dequantized_tensor, dtype = torch.int8, n_bits = 8):\n",
    "    \"\"\"\n",
    "    A method that plots 4 matrices, the original tensor, the quantized tensor\n",
    "    the de-quantized tensor and the error tensor.\n",
    "    \"\"\"\n",
    "    # Get a figure of 4 plots\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(15, 4))\n",
    "\n",
    "    # Plot the first matrix\n",
    "    plot_matrix(original_tensor, axes[0], 'Original Tensor', cmap=ListedColormap(['white']))\n",
    "\n",
    "    # Get the quantization range and plot the quantized tensor\n",
    "    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n",
    "    plot_matrix(quantized_tensor, axes[1], f'{n_bits}-bit Linear Quantized Tensor', vmin=q_min, vmax=q_max, cmap='coolwarm')\n",
    "\n",
    "    # Plot the de-quantized tensors\n",
    "    plot_matrix(dequantized_tensor, axes[2], 'Dequantized Tensor', cmap='coolwarm')\n",
    "\n",
    "    # Get the quantization errors\n",
    "    q_error_tensor = abs(original_tensor - dequantized_tensor)\n",
    "    plot_matrix(q_error_tensor, axes[3], 'Quantization Error Tensor', cmap=ListedColormap(['white']))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dequantized_tensor - test_tensor).square().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quantization I: Get the Scale and Zero Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### a dummy tensor to test the implementation\n",
    "test_tensor=torch.tensor(\n",
    "    [[191.6, -13.5, 728.6],\n",
    "     [92.14, 295.5,  -184],\n",
    "     [0,     684.6, 245.5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Scale and Zero Point for Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_min = torch.iinfo(torch.int8).min\n",
    "q_max = torch.iinfo(torch.int8).max\n",
    "print(q_min, \"\\n\", q_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_min = test_tensor.min().item()\n",
    "r_max = test_tensor.max().item()\n",
    "print(r_min, \"\\n\", r_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = (r_max - r_min) / (q_max - q_min)\n",
    "scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_point = q_min - (r_min / scale)\n",
    "print(zero_point)\n",
    "zero_point = int(round(zero_point))\n",
    "print(zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_scale_and_zero_point(tensor, dtype=torch.int8):\n",
    "    \n",
    "    q_min, q_max = torch.iinfo(dtype).min, torch.iinfo(dtype).max\n",
    "    r_min, r_max = tensor.min().item(), tensor.max().item()\n",
    "\n",
    "    scale = (r_max - r_min) / (q_max - q_min)\n",
    "\n",
    "    zero_point = q_min - (r_min / scale)\n",
    "\n",
    "    # clip the zero_point to fall in [quantized_min, quantized_max]\n",
    "    if zero_point < q_min:\n",
    "        zero_point = q_min\n",
    "    elif zero_point > q_max:\n",
    "        zero_point = q_max\n",
    "    else:\n",
    "        # round and cast to int\n",
    "        zero_point = int(round(zero_point))\n",
    "    \n",
    "    return scale, zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_scale, new_zero_point = get_q_scale_and_zero_point(test_tensor)\n",
    "print(new_scale, \"\\n\", new_zero_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization and Dequantization with Calculated Scale and Zero Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor = linear_q_with_scale_and_zero_point(test_tensor, new_scale, new_zero_point)\n",
    "dequantized_tensor = linear_dequantization(quantized_tensor, new_scale, new_zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quantization_errors(test_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dequantized_tensor-test_tensor).square().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_quantization(tensor, dtype=torch.int8):\n",
    "    scale, zero_point = get_q_scale_and_zero_point(tensor, \n",
    "                                                   dtype=dtype)\n",
    "    \n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,\n",
    "                                                          scale, \n",
    "                                                          zero_point, \n",
    "                                                          dtype=dtype)\n",
    "    \n",
    "    return quantized_tensor, scale , zero_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tensor = torch.randn((4, 4))\n",
    "r_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor, scale, zero_point = linear_quantization(r_tensor)\n",
    "print(quantized_tensor, \"\\n\", scale, \"\\n\", zero_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantized_tensor = linear_dequantization(quantized_tensor,\n",
    "                                           scale, zero_point)\n",
    "dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quantization_errors(r_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dequantized_tensor-r_tensor).square().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quantization: Symmetric Mode\n",
    "\n",
    "In asymmetric the r_min is mapped to q_min and r_max is mapped to q_max \n",
    "\n",
    "However in symmetric quantization we map -r_max to -q_max and r_max to q_max. r_max is max(abs(tensor)). Since the floating and quantized ranges are symmetric therefore zero is always zero. Hence this type of quantization has only scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_scale_symmetric(tensor, dtype=torch.int8):\n",
    "    r_max = tensor.abs().max().item()\n",
    "    q_max = torch.iinfo(dtype).max\n",
    "\n",
    "    # return the scale\n",
    "    return r_max/q_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test the implementation on a 4x4 matrix\n",
    "test_tensor = torch.randn((4, 4))\n",
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_q_scale_symmetric(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_q_symmetric(tensor, dtype=torch.int8):\n",
    "    scale = get_q_scale_symmetric(tensor)\n",
    "    \n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(tensor,\n",
    "                                                     scale=scale,\n",
    "                   # in symmetric quantization zero point is = 0    \n",
    "                                                    zero_point=0,\n",
    "                                                      dtype=dtype)\n",
    "    \n",
    "    return quantized_tensor, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor, scale = linear_q_symmetric(test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantized_tensor = linear_dequantization(quantized_tensor,scale,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quantization_errors(\n",
    "    test_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(tensor, dequantized_tensor):\n",
    "    return (dequantized_tensor - tensor).abs().square().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Quantization Error : \\\n",
    "{quantization_error(test_tensor, dequantized_tensor)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granularity of quantization\n",
    "All above was Per Tensor quantization i.e. the entire tensor will have one scale and zero point.\n",
    "\n",
    "Other types of granulairty are,\n",
    "- Per channel: where each axis is quantized separately i.e. has it's own scale and zero point\n",
    "- Per Group: Where a group of numbers are quantized separately i.e. has it's own scale and zero point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quantization II: Per Channel Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_q_symmetric_per_channel(r_tensor,dim,dtype=torch.int8):\n",
    "    output_dim = r_tensor.shape[dim]\n",
    "    scale = torch.zeros(output_dim)\n",
    "    for index in range(output_dim):\n",
    "        sub_tensor = r_tensor.select(dim,index)\n",
    "        # print(sub_tensor)\n",
    "        scale[index] = get_q_scale_symmetric(sub_tensor, dtype=dtype)\n",
    "    # reshape the scale\n",
    "    scale_shape = [1] * r_tensor.dim()\n",
    "    scale_shape[dim] = -1\n",
    "    scale = scale.view(scale_shape)\n",
    "    quantized_tensor = linear_q_with_scale_and_zero_point(\n",
    "        r_tensor, scale=scale, zero_point=0, dtype=dtype)\n",
    "    return quantized_tensor, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor=torch.tensor(\n",
    "    [[191.6, -13.5, 728.6],\n",
    "     [92.14, 295.5,  -184],\n",
    "     [0,     684.6, 245.5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### along the rows (dim = 0)\n",
    "quantized_tensor_0, scale_0 = linear_q_symmetric_per_channel(\n",
    "    test_tensor, dim=0)\n",
    "\n",
    "### along the columns (dim = 1)\n",
    "quantized_tensor_1, scale_1 = linear_q_symmetric_per_channel(\n",
    "    test_tensor, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantized_tensor_0 = linear_dequantization(\n",
    "    quantized_tensor_0, scale_0, 0)\n",
    "\n",
    "plot_quantization_errors(\n",
    "    test_tensor, quantized_tensor_0, dequantized_tensor_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Quantization Error : \\\n",
    "{quantization_error(test_tensor, dequantized_tensor_0)}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantized_tensor_1 = linear_dequantization(\n",
    "    quantized_tensor_1, scale_1, 0)\n",
    "\n",
    "plot_quantization_errors(\n",
    "    test_tensor, quantized_tensor_1, dequantized_tensor_1, n_bits=8)\n",
    "\n",
    "print(f\"\"\"Quantization Error : \\\n",
    "{quantization_error(test_tensor, dequantized_tensor_1)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Quantization II: Per Group Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_q_symmetric_per_group(tensor, group_size,\n",
    "                                 dtype=torch.int8):\n",
    "    \n",
    "    t_shape = tensor.shape\n",
    "    assert t_shape[1] % group_size == 0  # Only rows are grouped and quantized\n",
    "    assert tensor.dim() == 2\n",
    "    \n",
    "    tensor = tensor.view(-1, group_size)\n",
    "    \n",
    "    quantized_tensor, scale = linear_q_symmetric_per_channel(\n",
    "                                tensor, dim=0, dtype=dtype)\n",
    "    \n",
    "    quantized_tensor = quantized_tensor.view(t_shape)\n",
    "    \n",
    "    return quantized_tensor, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_dequantization_per_group(quantized_tensor, scale, \n",
    "                                    group_size):\n",
    "    \n",
    "    q_shape = quantized_tensor.shape\n",
    "    quantized_tensor = quantized_tensor.view(-1, group_size)\n",
    "    \n",
    "    dequantized_tensor = linear_dequantization(quantized_tensor, \n",
    "                                               scale, 0)\n",
    "    \n",
    "    dequantized_tensor = dequantized_tensor.view(q_shape)\n",
    "    \n",
    "    return dequantized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensor = torch.rand((6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_tensor, scale = linear_q_symmetric_per_group(\n",
    "    test_tensor, group_size=group_size)\n",
    "\n",
    "dequantized_tensor = linear_dequantization_per_group(\n",
    "    quantized_tensor, scale, group_size=group_size)\n",
    "\n",
    "plot_quantization_errors(\n",
    "    test_tensor, quantized_tensor, dequantized_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"Quantization Error : \\\n",
    "{quantization_error(test_tensor, dequantized_tensor)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either quantize the weights or we can quantize both weights and activations.\n",
    "\n",
    "- When we quantize weights only then we need to dequantize before performing the multiplication with activation.\n",
    "- We can also quanitze the weights and activations. Quantizing both results in integer multiplication and addition which is not supported by all hardwares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_linear_W8A32_without_bias(input, q_w, s_w, z_w):\n",
    "    assert input.dtype == torch.float32\n",
    "    assert q_w.dtype == torch.int8\n",
    "\n",
    "    dequantized_weight = q_w.to(torch.float32) * s_w + z_w\n",
    "    output = torch.nn.functional.linear(input, dequantized_weight)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "weight = torch.tensor([[-2,   -1.13, 0.42],\n",
    "                       [-1.51, 0.25, 1.62],\n",
    "                       [0.23,  1.35, 2.15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_w, s_w  = linear_q_symmetric(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = quantized_linear_W8A32_without_bias(input,\n",
    "                                             q_w,\n",
    "                                             s_w,\n",
    "                                             0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This is the W8A32 output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp32_output = torch.nn.functional.linear(input, weight)\n",
    "print(f\"This is the output if we don't quantize: {fp32_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Build an 8-Bit Quantizer\n",
    "\n",
    "For inference only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int8 = torch.randint(-128, 127, (32, 16)).to(torch.int8)\n",
    "random_hs = torch.randn((1, 16), dtype=torch.bfloat16)\n",
    "scales = torch.randn((1, 32), dtype=torch.bfloat16)\n",
    "bias = torch.randn((1, 32), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w8_a16_forward(weight, input, scales, bias=None):\n",
    "    \n",
    "    casted_weights = weight.to(input.dtype)\n",
    "    output = F.linear(input, casted_weights) * scales\n",
    "    \n",
    "    if bias is not None:\n",
    "        output = output + bias\n",
    "      \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"With bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales, bias))\n",
    "\n",
    "print(\"\\nWithout bias:\\n\\n\", \n",
    "      w8_a16_forward(random_int8, random_hs, scales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W8A16LinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, \n",
    "                 bias=True, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"int8_weights\",\n",
    "            torch.randint(\n",
    "                -128, 127, (out_features, in_features), dtype=torch.int8\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.register_buffer(\"scales\", \n",
    "                             torch.randn((out_features), dtype=dtype))\n",
    "        \n",
    "        if bias:\n",
    "            self.register_buffer(\"bias\", \n",
    "                                 torch.randn((1, out_features), \n",
    "                                             dtype=dtype))\n",
    "        \n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def quantize(self, weights):\n",
    "        w_fp32 = weights.clone().to(torch.float32)\n",
    "\n",
    "        scales = w_fp32.abs().max(dim=-1).values / 127\n",
    "        scales = scales.to(weights.dtype)\n",
    "\n",
    "        int8_weights = torch.round(weights\n",
    "                        /scales.unsqueeze(1)).to(torch.int8)\n",
    "\n",
    "        self.int8_weights = int8_weights\n",
    "        self.scales = scales\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return w8_a16_forward(self.int8_weights, \n",
    "                              input, self.scales, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = W8A16LinearLayer(4, 8)\n",
    "print(\"Weights before:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_matrix = torch.randn((4, 8), dtype=torch.bfloat16)\n",
    "module.quantize(random_matrix)\n",
    "print(\"Weights After:\\n\" , module.int8_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(module.scales)\n",
    "print(module.scales.shape)\n",
    "print(module.int8_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(random_matrix - module.int8_weights \n",
    " * module.scales.unsqueeze(1)).abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace PyTorch layers with Quantized Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_target_and_quantize(module, \n",
    "                               target_class, module_name_to_exclude):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear) and not \\\n",
    "        any([x == name for x in module_name_to_exclude]):\n",
    "            old_bias = child.bias\n",
    "            old_weight = child.weight\n",
    "\n",
    "            new_module = target_class(child.in_features, \n",
    "                                      child.out_features, \n",
    "                                      old_bias is not None, \n",
    "                                      child.weight.dtype)\n",
    "            setattr(module, name, new_module)\n",
    "\n",
    "            getattr(module, name).quantize(old_weight)\n",
    "            \n",
    "            if old_bias is not None:\n",
    "              getattr(module, name).bias = old_bias\n",
    "        else:\n",
    "            # Recursively call the function for nested modules\n",
    "            replace_linear_with_target_and_quantize(child, \n",
    "                     target_class, module_name_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.emb = torch.nn.Embedding(1, 1)\n",
    "    # Try with bias\n",
    "    self.linear_1 = nn.Linear(1, 1)\n",
    "    # Try without bias\n",
    "    self.linear_2 = nn.Linear(1, 1, bias=False)\n",
    "    # Lm prediction head\n",
    "    self.lm_head = nn.Linear(1, 1, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_linear_with_target_and_quantize(model_1, W8A16LinearLayer, [\"lm_head\"])\n",
    "print(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packing 2-bit Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Tensor: [1, 0, 3, 2]\n",
    "    # 1 0 3 2 - 01 00 11 10\n",
    "\n",
    "    # Starting point of packed int8 Tensor\n",
    "    # [0000 0000]\n",
    "\n",
    "    ##### First Iteration Start:\n",
    "    # packed int8 Tensor State: [0000 0000]\n",
    "    # 1 = 0000 0001\n",
    "    # 0000 0001\n",
    "    # No left shifts in the First Iteration\n",
    "    # After bit-wise OR operation between 0000 0000 and 0000 0001:\n",
    "    # packed int8 Tensor State: 0000 0001\n",
    "    ##### First Iteration End\n",
    "\n",
    "    ##### Second Iteration Start:\n",
    "    # packed int8 Tensor State: [0000 0001]\n",
    "    # 0 = 0000 0000\n",
    "    # 0000 0000\n",
    "    # 2 left shifts:\n",
    "    # [0000 0000] (1 shift)-> 0000 0000 (2 shift)-> 0000 0000\n",
    "    # After bit-wise OR operation between 0000 0001 and 0000 0000:\n",
    "    # packed int8 Tensor State: 0000 0001\n",
    "    ##### Second Iteration End\n",
    "\n",
    "    ##### Third Iteration Start:\n",
    "    # packed int8 Tensor State: [0000 0001]\n",
    "    # 3 = 0000 0011\n",
    "    # 0000 0011\n",
    "    # 4 left shifts:\n",
    "    # [0000 0011] (1 shift)-> 0000 0110 (2 shift)-> 0000 1100\n",
    "    # 0000 1100 (3 shift)-> 0001 1000 (4 shift)-> 0011 0000\n",
    "    # After bit-wise OR operation between 0000 0001 and 0011 0000:\n",
    "    # packed int8 Tensor State: 0011 0001\n",
    "    ##### Third Iteration End\n",
    "\n",
    "    ##### Fourth Iteration Start:\n",
    "    # packed int8 Tensor State: [0011 0001]\n",
    "    # 2 = 0000 0010\n",
    "    # 0000 0010\n",
    "    # 6 left shifts:\n",
    "    # [0000 0010] (1 shift)-> 0000 0100 (2 shift)-> 0000 1000\n",
    "    # 0000 1000 (3 shift)-> 0001 0000 (4 shift)-> 0010 0000\n",
    "    # 0010 0000 (5 shift)-> 0100 0000 (6 shift)-> 1000 0000\n",
    "    # After bit-wise OR operation between 0011 0001 and 1000 0000:\n",
    "    # packed int8 Tensor State: 1011 0001\n",
    "    ##### Fourth Iteration End\n",
    "\n",
    "    # Final packed int8 Tensor State: [1011 0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_weights(uint8tensor, bits):\n",
    "    if uint8tensor.shape[0] * bits % 8 != 0:\n",
    "        raise ValueError(f\"The input shape needs to be a mutiple \\\n",
    "        of {8 / bits} - got {uint8tensor.shape[0]}\")\n",
    "\n",
    "    num_values = uint8tensor.shape[0] * bits // 8\n",
    "\n",
    "    num_steps = 8 // bits\n",
    "\n",
    "    unpacked_idx = 0\n",
    "\n",
    "    packed_tensor = torch.zeros((num_values), dtype=torch.uint8)\n",
    "\n",
    "    # 1 0 3 2 - 01 00 11 10\n",
    "\n",
    "    # [0000 0000] -> 0000 0001\n",
    "\n",
    "    # 0000 0001\n",
    "\n",
    "    # 0000 0000 - 0000 0000\n",
    "\n",
    "    # 0000 0011 - 0011 0000 - 0011 0001\n",
    "\n",
    "    # 1011 0001\n",
    "    \n",
    "    for i in range(num_values):\n",
    "        for j in range(num_steps):\n",
    "            packed_tensor[i] |= uint8tensor[unpacked_idx] << (bits * j)\n",
    "            unpacked_idx += 1\n",
    "    return packed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_tensor = torch.tensor([1, 0, 3, 2], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_weights(unpacked_tensor, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_tensor = torch.tensor([1, 0, 3, 2, 3, 3, 3, 3], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pack_weights(unpacked_tensor, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_weights(uint8tensor, bits):\n",
    "    num_values = uint8tensor.shape[0] * 8 // bits\n",
    "\n",
    "    num_steps = 8 // bits\n",
    "\n",
    "    unpacked_tensor = torch.zeros((num_values), dtype=torch.uint8)\n",
    "\n",
    "    unpacked_idx = 0\n",
    "\n",
    "    # 1 0 3 2 - 01 00 11 10\n",
    "\n",
    "    # [00000000 00000000 00000000 00000000]\n",
    "    # [10110001 00101100 00001011 00000010]\n",
    "    # [00000001 00000000 00000011 00000010]\n",
    "\n",
    "    # 10110001\n",
    "    # 00000011\n",
    "    \n",
    "    # 00000001\n",
    "\n",
    "    # 1: [10110001]\n",
    "    # 2: [00101100]\n",
    "    # 3: [00001011]\n",
    "\n",
    "    mask = 2 ** bits - 1\n",
    "\n",
    "    for i in range(uint8tensor.shape[0]):\n",
    "        for j in range(num_steps):\n",
    "            unpacked_tensor[unpacked_idx] |= uint8tensor[i] >> (bits * j)\n",
    "            unpacked_idx += 1\n",
    "\n",
    "    unpacked_tensor &= mask\n",
    "    return unpacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked_tensor = torch.tensor([177, 255], dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer should be: torch.tensor([1, 0, 3, 2, 3, 3, 3, 3]\n",
    "unpack_weights(unpacked_tensor, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
